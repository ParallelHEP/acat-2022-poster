<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Performance study of HEP data analysis tools</title>
  <link rel="stylesheet" href="poster.css">
  <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1">
  <!-- Based on a poster template from https://github.com/cpitclaudel/academic-poster-template -->

  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link
    href="https://fonts.googleapis.com/css2?family=Fira+Sans+Condensed:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&amp;family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap"
    rel="stylesheet">

  <style type="text/css">
    html {
      font-size: 1.15rem
    }
  </style>
</head>

<body vocab="http://schema.org/" typeof="ScholarlyArticle">
  <header role="banner">
    <aside>
      <!-- <a href="https://github.com/cpitclaudel/academic-poster-template"><img src="logo.svg" alt="Tutorial logo"></a> -->
    </aside>
    <div>
      <h1 property="headline">Performance studies of software tools 
for an end-user HEP data analysis workflow: Uproot vs RDataFrame</h1>
      <!-- <h2 property="alternativeHeadline">
        Project carried out at INFN-Sezione di Bari (June - August 2022)
        as part of the DOE-INFN Summer Exchange Program 2022
      </h2> -->
      <!-- <h2 property="alternativeHeadline">
        Parallel ROOT I/O with Uproot and RDataFrame
      </h2> -->
      <address>
        <a property="author">Dung Hoang<sup>a</sup></a><a property="author">, Adriano Di Florio<sup>b, d</sup></a></a><a
          property="author">, Alexis Pompili<sup>b, c</sup></a></a><a property="author">, Umit Sozbilir<sup>b,
            c</sup></a></a><a property="author">, Vincenzo Mastrapasqua<sup>b, c</sup></a>
        <br /> <sup>a</sup><a property="sourceOrganization">Rhodes College,&nbsp</a><sup>b</sup><a
          property="sourceOrganization">INFN-Bari,&nbsp</a><sup>c</sup><a property="sourceOrganization">Università degli
          studi di Bari Aldo Moro,&nbsp</a><sup>d</sup><a property="sourceOrganization">Politecnico di Bari</a>
      </address>
      <span class="publication-info">
        <span property="publisher">Unpublished</span>,
        <time pubdate property="datePublished" datetime="2020-09-08">September 8, 2020</time>
      </span>
    </div>
    <aside>
      <!-- <a href="https://csail.mit.edu"><img style="background: white" src="csail.png" alt="MIT CSAIL Logo"></a> -->
    </aside>
  </header>

  <main property="articleBody">
    <article property="abstract">
<!------------------------------------------------------------>
      <header>
        <h3>Introduction</h3>
      </header>

      <p>
        The Python programming language is currently characterized by an enormous and rapidly growing “ecosystem” of
        software tools that support the analysis and visualization of “Big Data,” datasets that are too large or
        complex to be dealt with by traditional data-processing methods. Examples include <a
          href="https://numpy.org/">Numpy</a> (numerical
        computing), <a href="https://pandas.pydata.org/">pandas</a> (data manipulation), <a
          href="https://matplotlib.org/">matplotlib</a> (visualization), and <a
          href="https://www.tensorflow.org/">tensorflow</a> (machine learning). With the huge and increasingly complex
        volume of data that large-scale High Energy Physics (HEP) experiments
        produce, a complete HEP
        data analysis workflow can provide great benefits.
      </p>
      <p>
        In this project, we explored different Python-based
        approaches to deal with the
        standard ROOT format common to all HEP data. With reference to a representative HEP workflow taken as use case,
        we measured the performance of <strong><a href="https://uproot.readthedocs.io/en/latest/">Uproot</a></strong>, a
        library for reading and writing ROOT
        files in pure Python and NumPy, and of <strong><a
            href="https://root.cern/doc/master/classROOT_1_1RDataFrame.html">RDataFrame</a></strong>, the modern ROOT's
        high-level interface for efficient data
        analysis. To match the performance of a traditional C++ ROOT workflow, we also adopted parallel processing in
        our study.
      </p>
    </article>
<!------------------------------------------>
    <article property="abstract">
      <header>
        <h3>Data sample and HW setup</h3>
      </header>
      <p>
        The ROOT files used in our study, which have a total size of about 128GB, were taken from CMS Open Data.
      </p>
      <p>
        Measurements were run on three different machines on the ReCaS-Bari computing center:
      </p>

      <ol>
        <li>
          <strong>wn-gpu-8-3-22</strong> (256 CPUs, ~2000 GB RAM)
        </li>
        <li>
          <strong>wn-1-8-9</strong> (64 CPUs, ~250GB RAM)
        </li>
        <li>
          <strong>tesla04</strong> (32 CPUs, ~250 GB RAM)
        </li>
      </ol>

      <p>
In these tests:
<br>
        <ol>
 <li>         
these machine are dedicated exclusively for the performance tests in order to get unbiased results; <\li>
<li>
the data files are accessed remotely (unless otherwise stated). 
    Only on tesla04 the data can be stored locally and it was possible to carry out locally vs remotely stored data performance tests; <\li>
<li> the used ROOT version is 6.24; to address the study of RDF parallelization we also performed a comparison with 6.26 (on tesla04).<\li>
      </ol>
      </p>
    </article>
<!------------------------------------------>

    <article property="abstract">
      <header>
        <h3>Analysis workflow</h3>
      </header>

      <p>
        The analysis task consists in measuring the total runtime of the sequence of the following operations:
      <ol>
        <li> accessing the TTree inside the input ROOT files; </li>
        <li> applying specific filters (selection criteria) on the variables stored in the TTree, in order to extract a
          known physical signal associated to the decay chain of a beauty meson (B^0_s → J/psi phi, J/psi→mumu, phi→KK);
        </li>
        <li> converting the TBranch containing the selected invariant mass to a NumPy array for further analysis task (signal fitting, etc ...). </li>
      </ol>
      </p>

      <figure>
        <img src="plots/dist1.png" style="width:80%">
      </figure>
      <figure>
        <img src="plots/dist2.png" style="width:80%">
      </figure>
    </article>
<!------------------------------------------>







    <article property="abstract">
      <header>
        <h3>Parallel processing</h3>
      </header>

      <p>
        Instead of letting one CPU handle the whole dataset, we used multiple CPUs running in parallel to process data chunks concurrently.
      </p>
      <p>
        Without parallelism, the time needed for the analysis task to be completed would be:
      </p>

      \[T = t_u\cdot S\]

      <p>
        Where T is the total run time, t<sub>u</sub> is the time to process one unit of data, and S is the overall data size.
      </p>

      <p>
        By implementing parallel processing, this equation becomes:
      </p>

      \[ T = C(S, P) + t_u\cdot \lceil{S \over P}\rceil \]

      <p>
        where C(S, P) is the time to access/split/merge the data and P is the number of concurrent processes. The fraction S/P is rounded up to make sure that no data is lost.
      </p>
<!------------------------------------------>

    </article>

    <article property="abstract">
      <header>
        <h3>Parallel processing with Uproot</h3>
      </header>

      <p>
        Uproot does not provide a built-in option for implicit parallel processing. To enable parallelism, therefore, we manually split the data into smaller chunks and then create subprocesses with Python’s multiprocessing module. Since Uproot allows users to specify the number of events from each TTree to be processed,
        the smallest unit of data assigned to each subprocess can be one event. As a result, data can be distributed evenly among subprocesses, and the runtime function for Uproot would be:
      </p>

      \[ T = C(S, P) + t_e\cdot \lceil{S_e \over P}\rceil \]

      <p>
        where t<sub>e</sub> is the time to process one <strong>event</strong>, and S<sub>e</sub> is number of
        <strong>events</strong> in the dataset.
      </p>

    </article>
<!------------------------------------------>

<!--  commenting out this part
    <article property="abstract">
      <header>
        <h3>Parallel processing with RDataFrame</h3>
      </header>

      <p>
        RDataFrame, on the other hand, has a built-in option for implicit parallelism: EnableImplicitMT(). However, it
        appears to work inconsistently on different machines and different ROOT versions.
      </p>

      <figure>
        <img src="plots/implicitmt1.png">
      </figure>
      <figure>
        <img src="plots/implicitmt3.png">
      </figure>
      <figure>
        <img src="plots/implicitmt2.png">
      </figure>

    </article>
–>
<!------------------------------------------>

<!--
    <article property="abstract">
      <header>
        <h3>Parallel processing with RDataFrame </h3>
      </header>

      <p>
        Alternatively, we followed the same approach mentioned above: explicitly creating subprocesses to handle pieces of data. 
       –>
Unlike Uproot, RDF does not provide the option to specify the number of events to be processed in each file, and can only be parallelized over ROOT files. As a result, the runtime function for RDataFrame would be:
      </p>

      \[ T = C(S, P) + t_f\cdot \lceil{S_f \over P}\rceil \]

      <p>
        Where t<sub>f</sub> is the time to process one <strong>ROOT file</strong>, and S<sub>f</sub> is number of
        <strong>ROOT files</strong> in the dataset.
      </p>
<p>
In other words, each subprocess works on approximately the same number of files rather than the same number of events and thus, in principle, the workload is not divided equally. For example, if we have 5 files but only 4 subprocesses, one subprocess will have to deal with 2 files. Moreover, even when every subprocess handles exactly the same number of files, an uneven workload distribution will happen if the files have different sizes. In both cases the overall runtime of the task is fatefully limited by the slowest subprocess.
</p>

      <p>
       To mitigate this current RDF issue, with the aim to set up a fair performance comparison, by means of Uproot we preliminarily split the original dataset into 128 files, each containing the same number of events. We used these files as input for both Uproot and RDF performance studies.
      </p>

    </article>
<!------------------------------------------>







<article property="abstract">
      <header>
        <h3>Preliminary studies: local vs remote data access</h3>
      </header>

<p>
It has been checked that accessing data locally and remotely (on the same machine tesla04) has marginal impact on the runtime: effect is minor for RDF (with slight preference - of course - for local storage) and negligible for Uproot. 
</p>

      <figure>
        <img src="plots/localvsremote1.png">
      </figure>
      <figure>
        <img src="plots/localvsremote2.png">
      </figure>

      <p>
Thus the main studies are carried out by accessing data remotely, without biasing the performance comparison. 
      </p>
    </article>
   <!------------------------------------------>


 <article property="abstract">
      <header>
        <h3>Preliminary studies: ROOT 6.24 vs 6.26</h3>
      </header>
<p>
RDF has a built-in option for parallel processing: EnableImplicitMT(). We experienced an unstable behavior - on ROOT 6.24 - when letting this method autonomously optimize the number of threads. We have checked that upgrading to ROOT 6.26 this issue disappears: 
</p>
     <figure>
        <img src="plots/implicitmt2.png">
      </figure>

      <p>In the main tests we manually set the number of processes by setting n_threads in the method: EnableImplicitMT(n_htreads).
</p>

    </article>
 <!------------------------------------------>


    <article property="abstract">
      <header>
        <h3>Results : Runtime vs Processes</h3>
      </header>

      <p>
        By keeping the size of data constant (128 files/128GB/4.5 &#183; 10<sup>8</sup> events) we obtain the following runtime as a function of the number of processes, for each used server:
      </p>

      <figure>
        <img src="plots/rvp1.png">
      </figure>
      <figure>
        <img src="plots/rvp2.png">
      </figure>
      <figure>
        <img src="plots/rvp3.png">
      </figure>

      <p>
        The plots show that runtime decreases with the increasing number of processes and saturates when hitting the about constant time C(S, P) needed for initialization and final merging.
      <br>
While the Uproot curves are fairly smooth, we can appreciate a discrete pattern in RDF ones. This reflects the fact that RDF’s smallest unit of data is one file and not just one event.
      </p>
    </article>

 <!------------------------------------------>

    <article property="abstract">
      <header>
        <h3>Results : Speed-up vs Processes</h3>
      </header>
      <p>
        From the data in the previous plots, we can estimate the speed-up, the quantity defined as the ratio between the runtime for 1 process and the overall runtime for n processes.
      </p>
      <figure>
        <img src="plots/svp1.png">
      </figure>
      <figure>
        <img src="plots/svp2.png">
      </figure>
      <figure>
        <img src="plots/svp3.png">
      </figure>

      <p>
        It can be observed that there is a significant performance boost due to parallel processing, both with Uproot and RDF. On the most powerful server (wn-gpu-8-3-22), for instance, Uproot with multiprocessing is almost 9 times faster than for a single process. In the case of RDataFrame, we achieved an even more striking speed-up: at 128 subprocesses, it was beyond 50 times faster.
      </p>

      <p>
        As expected, the saturation effect occurs when running with more subprocesses than the number of CPUs available on the machine. This is clearly visible on wn-1-8-9 and tesla04, for which the number of CPUs is smaller.

With Uproot, however, the performance boost always peaks at around 32 processes, regardless the amount of available CPUs. This seems to happen because - for Uproot - 
C(S, P) is enough large to dominantly contribute to the runtime.
Unlikely - in the case of RDF - C(S, P) is marginally contributing to the runtime, and the performance gain can grow until the maximum number of available CPUs is reached.
      </p>
    </article>

 <!------------------------------------------>


    <article property="abstract">
      <header>
        <h3>Results - Runtime vs Size</h3>
      </header>
      <p>
Another way of studying the performance consists in varying the size of data while keeping constant the number  of processes (32 or 64 has been chosen): 
     </p>
      <figure>
        <img src="plots/rvs1.png">
      </figure>
      <figure>
        <img src="plots/rvs2.png">
      </figure>
      <figure>
        <img src="plots/rvs3.png">
      </figure>

      <p>
        The total runtime is linearly proportional to the data size, as expected.
        In RDF curves, the observed step-like pattern seems to reflect that RDF
        is parallelized over files instead of events (reminder: each file has a size of ~1GB).
      </p>

<!------------------------------------------>

    </article>

    <article property="abstract">
      <header>
        <h3>Conclusions</h3>
      </header>

      <p>
        By enabling parallelism, the performance of both Uproot and RDataFrame are well enhanced. While Uproot runs faster at a fewer number of processes, RDataFrame reaches a better performance as the number of processes increases, thus suggesting its use when having at disposal a machine with a large number of CPUs. Note that RDF performances have been optimized by evenly splitting the original dataset into many files with the same number of events. In real-life situations, such condition can be reached either using Uproot or at the initial merging step applied to the rootuples obtained by the distributed experiment framework.
      </p>

    </article>

    <!-- <figure style="flex-grow: 9999999">
      <img style="width: 70%" src="logo.svg" alt="Project logo" />
      <figcaption>A stand-alone figure to fill the remaining space.</figcaption>
    </figure> -->
  </main>

  <footer>
    <address class="monospace">
      hoadh-23@rhodes.edu
    </address>
    <address class="monospace">
      adriano.diflorio@ba.infn.it
    </address>
    <address class="monospace">
      alexis.pompili@ba.infn.it
    </address>
    <address class="monospace">
      umit.sozbilir@cern.ch
    </address>
    <address class="monospace">
      vincenzo.mastrapasqua@cern.ch
    </address>
    <!-- <span class="credits">
    </span> -->
  </footer>
</body>

</html>

